{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">First of all -- Checking Questions</h1> \n",
    "\n",
    "**Вопрос 1**: Можно ли использовать сверточные сети для классификации текстов? Если нет обоснуйте :D, если да то как? как решить проблему с произвольной длинной входа?\n",
    "\n",
    "<Ответ> Да, можно использовать посимвольный подход.\n",
    "\n",
    "**Вопрос 2**: Чем LSTM лучше/хуже чем обычная RNN?\n",
    "\n",
    "<Ответ> при применении LSTM градиент не затухает(или делает это в пределах разумного), а также не взрывается, он сохраняется гораздо дольше. Но он требует больше памяти и вычислений.\n",
    "\n",
    "**Вопрос 3**:  Выпишите производную $\\frac{d c_{n+1}}{d c_{k}}$ для LSTM http://colah.github.io/posts/2015-08-Understanding-LSTMs/, объясните формулу, когда производная затухает, когда взрывается?\n",
    "\n",
    "<Ответ> https://github.com/g-n-ev/Machine-learning-course/tree/master/hw7/ans_3.jpg\n",
    "\n",
    "**Вопрос 4**: Зачем нужен TBPTT почему BPTT плох?\n",
    "\n",
    "<Ответ> BPTT проделывает больше операций, следовательно, требуется больше ресурсов. TBPTT работает быстее, можно применять для претрейнинга.\n",
    "\n",
    "\n",
    "**Вопрос 5**: Как комбинировать рекуррентные и сверточные сети, а главное зачем? Приведите несколько примеров реальных задач.\n",
    "\n",
    "<Ответ> Можно разбивать данные путем свертки, а затем обучать их с помощью рекуррентной сети. Это добавит скорости обаботки одного элемента, но может в целом потребовать больше ресурсов\n",
    "\n",
    "**Вопрос 6**: Объясните интуицию выбора размера эмбединг слоя? почему это опасное место?\n",
    "\n",
    "<Ответ> порядка процента от исходного словаря. Неаккуратный выбор размера может повлечь потерю большой части данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* Arseniy Ashuha, you can text me ```ars.ashuha@gmail.com```, Александр Панин\n",
    "\n",
    "<h1 align=\"center\"> Image Captioning </h1> \n",
    "\n",
    "In this seminar you'll be going through the image captioning pipeline. It can help u https://ars-ashuha.ru/slides/2016.11.11_ImageCaptioning/image_captionong.pdf \n",
    "\n",
    "To begin with, let us download the dataset of image features from a pre-trained GoogleNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n",
      "\"tar\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/3hj16b0fj6yw7cc/data.tar.gz?dl=1 -O data.tar.gz\n",
    "!tar -xvzf data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read Dataset\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "img_codes = np.load(\"data/image_codes.npy\")\n",
    "captions = pickle.load(open('data/caption_tokens.pcl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each image code is a 1000-unit vector: (123287L, 1000L)\n",
      "[ 1.38901556 -3.82951474 -1.94360816 -0.5317238  -0.03120959 -2.87483215\n",
      " -2.9554503   0.6960277  -0.68551242 -0.7855981 ]\n",
      "\n",
      "\n",
      "\n",
      "for each image there are 5-7 descriptions, e.g.:\n",
      "\n",
      "a man with a red helmet on a small moped on a dirt road\n",
      "man riding a motor bike on a dirt road on the countryside\n",
      "a man riding on the back of a motorcycle\n",
      "a dirt path with a young person on a motor bike rests to the foreground of a verdant area with a bridge and a background of cloud wreathed mountains\n",
      "a man in a red shirt and a red hat is on a motorcycle on a hill side\n"
     ]
    }
   ],
   "source": [
    "print \"each image code is a 1000-unit vector:\", img_codes.shape\n",
    "print img_codes[0,:10]\n",
    "print '\\n\\n'\n",
    "print \"for each image there are 5-7 descriptions, e.g.:\\n\"\n",
    "print '\\n'.join(captions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split descriptions into tokens\n",
    "for img_i in range(len(captions)):\n",
    "    for caption_i in range(len(captions[img_i])):\n",
    "        sentence = captions[img_i][caption_i] \n",
    "        captions[img_i][caption_i] = [\"#START#\"]+sentence.split(' ')+[\"#END#\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voc = []\n",
    "voc_all = []\n",
    "for img_i in range(len(captions)):\n",
    "    for caption_i in range(len(captions[img_i])):\n",
    "        for el in captions[img_i][caption_i]:\n",
    "            if el not in voc:\n",
    "                voc.append(el)\n",
    "            voc_all.append(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n"
     ]
    }
   ],
   "source": [
    "# Build a Vocabulary\n",
    "word_counts = dict()\n",
    "i = 0\n",
    "for word in voc:\n",
    "    word_counts[word] = voc_all.count(word)\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"vocad.txt\", 'r') as f:\n",
    "    voc = f.readlines()\n",
    "vocab = []\n",
    "for elem in voc:\n",
    "    vocab.append(elem[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"dictionary_keys.txt\", 'r') as f:\n",
    "    dic_k = f.readlines()\n",
    "dict_k = []\n",
    "for elem in dic_k:\n",
    "    dict_k.append(elem[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"dictionary_values.txt\", 'r') as f:\n",
    "    dic_v = f.readlines()\n",
    "dict_v = []\n",
    "for elem in dic_v:\n",
    "    dict_v.append(int(elem[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_counts = dict()\n",
    "for i in range(len(dict_k)):\n",
    "     word_counts[dict_k[i]] = dict_v[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab  = ['#UNK#', '#START#', '#END#']\n",
    "vocab += [k for k, v in word_counts.items() if v >= 5]\n",
    "n_tokens = len(vocab)\n",
    "\n",
    "assert 10000 <= n_tokens <= 10500\n",
    "\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_ix = -1\n",
    "UNK_ix = vocab.index('#UNK#')\n",
    "\n",
    "def as_matrix(sequences,max_len=None):\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences),max_len),dtype='int32')+PAD_ix\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [word_to_index.get(word,UNK_ix) for word in seq[:max_len]]\n",
    "        matrix[i,:len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8508,  8739,  8316,  6620,  1354,  8739,   133,  7732,  2469,\n",
       "         4928,  3249,  8168,    -1,    -1,    -1,    -1],\n",
       "       [ 8508,  8739,  6620,  1847,  8739,  9929,  5187,  5750, 10051,\n",
       "         8739,  3249,  8274, 10133,  4771,  8168,    -1],\n",
       "       [ 8508,  8739,  6620,  9088,  8739,  7479,  4928,  3249,  8274,\n",
       "        10133,  4771,  8168,    -1,    -1,    -1,    -1],\n",
       "       [ 8508,  8739,  6620,  1847, 10133,  5750,  5187,  9929,   132,\n",
       "         3169,  4928,  3249,  8274, 10133,  4771,  8168],\n",
       "       [ 8508,  8739,  6620,   328,   723,  6620,  8698,   132,  7513,\n",
       "         3249,  8274, 10133,  4771,  8168,    -1,    -1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try it out on several descriptions of a random image\n",
    "as_matrix(captions[1338])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys_str = []\n",
    "for elem in word_counts.keys():\n",
    "    keys_str.append(elem.encode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Сохранила предобработанные данные\n",
    "np.savetxt('dictionary_values.txt',word_counts.values(), fmt = ('%.18e'), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('dictionary_keys.txt',keys_str,fmt = '%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('vocad.txt', vocab, fmt= '%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mah Neural Networksa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# network shapes. \n",
    "CNN_FEATURE_SIZE = img_codes.shape[1]\n",
    "EMBED_SIZE = 128 #pls change me if u want\n",
    "LSTM_UNITS = 200 #pls change me if u want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GTA\\Anaconda2\\lib\\site-packages\\theano\\tensor\\signal\\downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import lasagne\n",
    "import theano.tensor as T\n",
    "from lasagne.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input Variable\n",
    "sentences = T.imatrix()# [batch_size x time] of word ids\n",
    "image_vectors = T.matrix() # [batch size x unit] of CNN image features\n",
    "sentence_mask = T.neq(sentences, PAD_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Shape.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_mask = T.neq(sentences, PAD_ix)\n",
    "words = InputLayer((None, None), sentences)\n",
    "masks = InputLayer((None, None), sentence_mask)\n",
    "words_emb = EmbeddingLayer(words, n_tokens, EMBED_SIZE)\n",
    "image_fea = InputLayer((None, CNN_FEATURE_SIZE), image_vec)\n",
    "image_emb = DropoutLayer(image_fea, 0.5)\n",
    "image_emb = DenseLayer(image_emb, LSTM_UNITS)\n",
    "decoder = LSTMLayer(\n",
    "words_emb, num_units=LSTM_UNITS,\n",
    "cell_init=image_emb, mask_input=mask)\n",
    "decoder = Reshape (...)\n",
    "loss = categorical_cross_enrtopy (...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#network inputs\n",
    "l_words = InputLayer((None, None), sentences)\n",
    "l_mask = InputLayer((None, None), sentence_mask)\n",
    "\n",
    "#embeddings for words \n",
    "############# TO CODE IT BY YOURSELF ##################\n",
    "l_word_embeddings = EmbeddingLayer(l_words, n_tokens, EMBED_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input layer for image features\n",
    "l_image_features = InputLayer((None, CNN_FEATURE_SIZE), image_vectors)\n",
    "\n",
    "############# TO CODE IT BY YOURSELF ##################\n",
    "#convert 1000 image features from googlenet to whatever LSTM_UNITS you have set\n",
    "#it's also a good idea to add some dropout here and there\n",
    "\n",
    "l_image_features_small = DropoutLayer(l_image_features, p = 0.5) #<Apply Dropout Layer to regularise your Net>\n",
    "l_image_features_small = DenseLayer(l_image_features_small,LSTM_UNITS) #<Apply Dense to acive LSTM_UNITS size of representation>\n",
    "assert l_image_features_small.output_shape == (None, LSTM_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lasagne.layers.dense.DenseLayer at 0x56be5208>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_image_features_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot initialize parameters: 'spec' is not a numpy array, a Theano shared variable, or a callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-57321dc724a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m                     \u001b[0mcell_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ml_image_features_small\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                     \u001b[0mmask_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ml_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                     grad_clipping=1)\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\GTA\\Anaconda2\\lib\\site-packages\\lasagne\\layers\\recurrent.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, incoming, num_units, ingate, forgetgate, cell, outgate, nonlinearity, cell_init, hid_init, backwards, learn_init, peepholes, gradient_steps, grad_clipping, unroll_scan, precompute_input, mask_input, **kwargs)\u001b[0m\n\u001b[0;32m    837\u001b[0m             self.cell_init = self.add_param(\n\u001b[0;32m    838\u001b[0m                 \u001b[0mcell_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cell_init\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 839\u001b[1;33m                 trainable=learn_init, regularizable=False)\n\u001b[0m\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhid_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GTA\\Anaconda2\\lib\\site-packages\\lasagne\\layers\\base.pyc\u001b[0m in \u001b[0;36madd_param\u001b[1;34m(self, spec, shape, name, **tags)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%s.%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mparam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m         \u001b[1;31m# parameters should be trainable and regularizable by default\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[0mtags\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'trainable'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'trainable'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GTA\\Anaconda2\\lib\\site-packages\\lasagne\\utils.pyc\u001b[0m in \u001b[0;36mcreate_param\u001b[1;34m(spec, shape, name)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         raise RuntimeError(\"cannot initialize parameters: 'spec' is not \"\n\u001b[0m\u001b[0;32m    313\u001b[0m                            \u001b[1;34m\"a numpy array, a Theano shared variable, or a \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                            \"callable\")\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cannot initialize parameters: 'spec' is not a numpy array, a Theano shared variable, or a callable"
     ]
    }
   ],
   "source": [
    "############# TO CODE IT BY YOURSELF ##################\n",
    "# Concatinate image features and word embedings in one sequence \n",
    "decoder = LSTMLayer(l_word_embeddings, \n",
    "                    num_units=LSTM_UNITS, \n",
    "                    cell_init=l_image_features_small, \n",
    "                    mask_input=l_mask,\n",
    "                    grad_clipping=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-03795674750a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#apply whatever comes next to each tick of each example in a batch. Equivalent to 2 reshapes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mbroadcast_decoder_ticks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBroadcastLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"broadcasted decoder shape = \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbroadcast_decoder_ticks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decoder' is not defined"
     ]
    }
   ],
   "source": [
    "# Decoding of rnn hiden states\n",
    "from broadcast import BroadcastLayer,UnbroadcastLayer\n",
    "\n",
    "#apply whatever comes next to each tick of each example in a batch. Equivalent to 2 reshapes\n",
    "broadcast_decoder_ticks = BroadcastLayer(decoder, (0, 1))\n",
    "print \"broadcasted decoder shape = \",broadcast_decoder_ticks.output_shape\n",
    "\n",
    "predicted_probabilities_each_tick = DenseLayer(\n",
    "    broadcast_decoder_ticks,n_tokens, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "#un-broadcast back into (batch,tick,probabilities)\n",
    "predicted_probabilities = UnbroadcastLayer(\n",
    "    predicted_probabilities_each_tick, broadcast_layer=broadcast_decoder_ticks)\n",
    "\n",
    "print \"output shape = \", predicted_probabilities.output_shape\n",
    "\n",
    "#remove if you know what you're doing (e.g. 1d convolutions or fixed shape)\n",
    "assert predicted_probabilities.output_shape == (None, None, 10373)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_probabilities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-a3df2ba2c806>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnext_word_probas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_probabilities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mreference_answers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predicted_probabilities' is not defined"
     ]
    }
   ],
   "source": [
    "next_word_probas = get_output(predicted_probabilities)\n",
    "\n",
    "reference_answers = sentences[:,1:]\n",
    "output_mask = sentence_mask[:,1:]\n",
    "\n",
    "#write symbolic loss function to train NN for\n",
    "loss = lasagne.objectives.categorical_crossentropy(\n",
    "    next_word_probas[:, :-1].reshape((-1, n_tokens)),\n",
    "    reference_answers.reshape((-1,))\n",
    ").reshape(reference_answers.shape)\n",
    "\n",
    "############# TO CODE IT BY YOURSELF ##################\n",
    "loss =  <mean over non-PAD tokens>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trainable NN weights\n",
    "############# TO CODE IT BY YOURSELF ##################\n",
    "weights = <all dnn weigts>\n",
    "updates = <your favorite optimizer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile a function that takes input sentence and image mask, outputs loss and updates weights\n",
    "#please not that your functions must accept image features as FIRST param and sentences as second one\n",
    "############# TO CODE IT BY YOURSELF ##################\n",
    "train_step = <>\n",
    "val_step   = <>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training\n",
    "\n",
    "* You first have to implement a batch generator\n",
    "* Than the network will get trained the usual way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "captions = np.array(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "def generate_batch(images,captions,batch_size,max_caption_len=None):\n",
    "    #sample random numbers for image/caption indicies\n",
    "    random_image_ix = np.random.randint(0, len(images), size=batch_size)\n",
    "    \n",
    "    #get images\n",
    "    batch_images = images[random_image_ix]\n",
    "    \n",
    "    #5-7 captions for each image\n",
    "    captions_for_batch_images = captions[random_image_ix]\n",
    "    \n",
    "    #pick 1 from 5-7 captions for each image\n",
    "    batch_captions = map(choice, captions_for_batch_images)\n",
    "    \n",
    "    #convert to matrix\n",
    "    batch_captions_ix = as_matrix(batch_captions,max_len=max_caption_len)\n",
    "    \n",
    "    return batch_images, batch_captions_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.60496843, -0.2175829 ,  2.45054078, ..., -3.86057615,\n",
       "          3.22173691, -1.05588841],\n",
       "        [-3.09767723, -1.64312673, -0.65251952, ..., -1.1166991 ,\n",
       "          2.94198656, -1.68168533],\n",
       "        [-2.60519528, -4.79385853, -0.87879199, ..., -4.22373724,\n",
       "          2.63850451,  3.32209778]], dtype=float32),\n",
       " array([[ 8508,  4983,  7179,  3511,  4558,  1644,  3396,  2108, 10336,\n",
       "          8168,    -1,    -1,    -1],\n",
       "        [ 8508,  8739,  3590,  7392,  5520,  3080,  1920,  5650,  2730,\n",
       "          3824,  8168,    -1,    -1],\n",
       "        [ 8508,   383,  1445,  4046,   206,  2366,  8996,  9833,  8274,\n",
       "          3396,   773, 10010,  8168]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_batch(img_codes,captions, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop\n",
    "* We recommend you to periodically evaluate the network using the next \"apply trained model\" block\n",
    " *  its safe to interrupt training, run a few examples and start training again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50 #adjust me\n",
    "n_epochs   = 100 #adjust me\n",
    "n_batches_per_epoch = 50 #adjust me\n",
    "n_validation_batches = 5 #how many batches are used for validation after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_step' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-4c0b798e217d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batches_per_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgenerate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_codes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mn_batches_per_epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_step' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss=0\n",
    "    for _ in tqdm(range(n_batches_per_epoch)):\n",
    "        train_loss += train_step(*generate_batch(img_codes,captions,batch_size))\n",
    "    train_loss /= n_batches_per_epoch\n",
    "    \n",
    "    val_loss=0\n",
    "    for _ in range(n_validation_batches):\n",
    "        val_loss += val_step(*generate_batch(img_codes,captions,batch_size))\n",
    "    val_loss /= n_validation_batches\n",
    "    \n",
    "    print('\\nEpoch: {}, train loss: {}, val loss: {}'.format(epoch, train_loss, val_loss))\n",
    "\n",
    "print(\"Finish :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### apply trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the same kind you did last week, but a bit smaller\n",
    "from pretrained_lenet import build_model,preprocess,MEAN_VALUES\n",
    "\n",
    "# build googlenet\n",
    "lenet = build_model()\n",
    "\n",
    "#load weights\n",
    "lenet_weights = pickle.load(open('data/blvc_googlenet.pkl'))['param values']\n",
    "set_all_param_values(lenet[\"prob\"], lenet_weights)\n",
    "\n",
    "#compile get_features\n",
    "cnn_input_var = lenet['input'].input_var\n",
    "cnn_feature_layer = lenet['loss3/classifier']\n",
    "get_cnn_features = theano.function([cnn_input_var], lasagne.layers.get_output(cnn_feature_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#sample image\n",
    "img = plt.imread('data/Dog-and-Cat.jpg')\n",
    "img = preprocess(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#deprocess and show, one line :)\n",
    "from pretrained_lenet import MEAN_VALUES\n",
    "plt.imshow(np.transpose((img[0] + MEAN_VALUES)[::-1],[1,2,0]).astype('uint8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_word_probas_det = get_output(predicted_probabilities,deterministic=False)[:,-1]\n",
    "\n",
    "get_probs = theano.function([image_vectors,sentences], last_word_probas_det)\n",
    "\n",
    "#this is exactly the generation function from week5 classwork,\n",
    "#except now we condition on image features instead of words\n",
    "def generate_caption(image,caption_prefix = (\"START\",),t=1,sample=True,max_len=100):\n",
    "    image_features = get_cnn_features(image)\n",
    "    caption = list(caption_prefix)\n",
    "    for _ in range(max_len):\n",
    "        \n",
    "        next_word_probs = get_probs(image_features,as_matrix([caption]) ).ravel()\n",
    "        #apply temperature\n",
    "        next_word_probs = next_word_probs**t / np.sum(next_word_probs**t)\n",
    "\n",
    "        if sample:\n",
    "            next_word = np.random.choice(vocab,p=next_word_probs) \n",
    "        else:\n",
    "            next_word = vocab[np.argmax(next_word_probs)]\n",
    "\n",
    "        caption.append(next_word)\n",
    "\n",
    "        if next_word==\"#END#\":\n",
    "            break\n",
    "            \n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print ' '.join(generate_caption(img,t=1.)[1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Part\n",
    "- Use ResNet Instead of GoogLeNet\n",
    "- Use W2V as embedding\n",
    "- Use Attention :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass Assignment https://goo.gl/forms/2qqVtfepn0t1aDgh1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
